{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora Quest Build Embeddings using word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachin-cw/Springboard-Capstone/blob/master/Quora_Quest_Build_Embeddings_using_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttDeHE4RlIy6",
        "colab_type": "text"
      },
      "source": [
        "#Experiment training word2vec on the dataset to build our own embeddings or simply put word vectors using the Gensim implementation of Word2Vec. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxIwhQErmvh0",
        "colab_type": "text"
      },
      "source": [
        "Load all the dependencies and mount the Google drive to load the training file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FShlDUcnBaLr",
        "colab_type": "code",
        "outputId": "7d4e0c77-2298-48b2-af5f-aaf79b98275d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "# above lines get me the cell execution time.\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "#above lines help me avoid warnings thrown by sklearn\n",
        "\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load stop words\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.remove('no')\n",
        "stop_words.remove('not')\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('gdrive/My Drive/kaggle/quora')\n",
        "!ls -ltr\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/f9/0626bbdb322e3a078d968e87e3b01341e7890544de891d0cb613641220e6/ipython-autotime-0.1.tar.bz2\n",
            "Building wheels for collected packages: ipython-autotime\n",
            "  Building wheel for ipython-autotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-cp36-none-any.whl size=1832 sha256=7d3ad6856c146893a1c3c020094ce37aab54d99925e3b947b3e58c3379fc6061\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/df/81/2db1e54bc91002cec40334629bc39cfa86dff540b304ebcd6e\n",
            "Successfully built ipython-autotime\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "total 12052983\n",
            "-rw------- 1 root root 5646236541 Oct 24  2015 glove.840B.300d.txt\n",
            "-rw------- 1 root root 2259088777 Mar 14  2018 wiki-news-300d-1M.vec\n",
            "-rw------- 1 root root 3644258522 Oct 31  2018 GoogleNews-vectors-negative300.bin\n",
            "drwx------ 2 root root       4096 Aug 25 19:03 paragram_300_sl999\n",
            "-rw------- 1 root root   35011536 Aug 25 19:25 test.csv\n",
            "-rw------- 1 root root  124206772 Aug 25 19:25 train.csv\n",
            "-rw------- 1 root root    8643553 Aug 25 19:26 sample_submission.csv\n",
            "-rw------- 1 root root    6617121 Aug 28 11:36 big.txt\n",
            "drwx------ 2 root root       4096 Sep  2 19:02 catboost_info\n",
            "-rw------- 1 root root    8643553 Sep  7 22:19 submission.csv\n",
            "-rw------- 1 root root  186744887 Sep  8 17:28 quora.csv\n",
            "-rw------- 1 root root  177407020 Sep 21 18:00 quora_preprocessed_stemmed.csv\n",
            "-rw------- 1 root root  116664448 Oct 10 19:29 weights.hdf5\n",
            "-rw------- 1 root root  116664480 Oct 10 20:02 treated_weights.hdf5\n",
            "drwx------ 2 root root       4096 Oct 10 21:03 models\n",
            "-rw------- 1 root root   12051265 Oct 23 22:28 quora_classification_model.sav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEo-e0Cwm1cl",
        "colab_type": "text"
      },
      "source": [
        "Dictionary below will be used to expand the contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44xsz3bJBiEE",
        "colab_type": "code",
        "outputId": "9a030e6a-8af4-4310-e36d-e6f943e0047e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 133 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9OLKK0AnBMr",
        "colab_type": "text"
      },
      "source": [
        "Next up are functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1uas6rvBsEc",
        "colab_type": "code",
        "outputId": "67258566-694a-42a2-a2f1-60a4d02998cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import time\n",
        "\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "  \n",
        "def remove_special_chars(text):\n",
        "  return re.sub(r'[^a-zA-Z0-9 \\']', '', text)\n",
        "\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  return tokens\n",
        "\n",
        "def normalize_text(text):\n",
        "  text=remove_special_chars(text)\n",
        "  text=expand_contractions(text)\n",
        "  text=text.lower()\n",
        "  text=remove_accented_chars(text)\n",
        "  #text=tokenize_text(text)\n",
        "  return text\n",
        "\n",
        "def remove_stopwords_nltk(tokens):\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  filtered_text = ' '.join(filtered_tokens)    \n",
        "  return filtered_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def stem_sentences_nltk(sentence):\n",
        "  ps = PorterStemmer()\n",
        "  tokens = sentence.split()\n",
        "  stemmed_tokens = [ps.stem(token) for token in tokens]\n",
        "  return ' '.join(stemmed_tokens)\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "def stem_sentences_list(sentence):\n",
        "  ps = PorterStemmer()\n",
        "  tokens = sentence.split()\n",
        "  stemmed_tokens = [ps.stem(token) for token in tokens]\n",
        "  return stemmed_tokens\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(df,stem=False):\n",
        "  if stem:\n",
        "    df['word_tokens_stemmed']=df['question_text'].apply(normalize_text).apply(tokenize_text).apply(remove_stopwords_nltk).apply(stem_sentences_nltk)\n",
        "  else:\n",
        "    df['word_tokens_preprocessed']=df['question_text'].apply(normalize_text).apply(tokenize_text).apply(remove_stopwords_nltk)\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_train_test(df,stem=False):\n",
        "  if stem:\n",
        "    x_train,x_test,y_train,y_test = train_test_split(df['word_tokens_stemmed'],df['target'], random_state=1,stratify=df['target'])\n",
        "  else:\n",
        "    x_train,x_test,y_train,y_test = train_test_split(df['word_tokens_preprocessed'],df['target'], random_state=1,stratify=df['target'])\n",
        "  return x_train,x_test,y_train,y_test\n",
        "\n",
        "def classifier_reports(y_test, y_pred):\n",
        "  print()\n",
        "  class_names = ['class0', 'class1']\n",
        "  print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "   \n",
        "  plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 106 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDA4LM4knQbq",
        "colab_type": "text"
      },
      "source": [
        "Load the training file and preprocess it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esrYAgWNByJR",
        "colab_type": "code",
        "outputId": "05d7c7af-dc34-4520-92cd-37f9788b3e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "\n",
        "df=pd.read_csv('train.csv')\n",
        "\n",
        "pd.options.display.max_colwidth = 500\n",
        "print('Number rows and columns:',df.shape)\n",
        "\n",
        "df=preprocess_text(df,stem=False)  # takes 5 min 39 secs to preprocess on full dataset of 1.3 million rows\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number rows and columns: (1306122, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "      <th>word_tokens_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00002165364db923c7e6</td>\n",
              "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
              "      <td>0</td>\n",
              "      <td>quebec nationalists see province nation 1960s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000032939017120e6e44</td>\n",
              "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
              "      <td>0</td>\n",
              "      <td>adopted dog would encourage people adopt not shop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000412ca6e4628ce2cf</td>\n",
              "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
              "      <td>0</td>\n",
              "      <td>velocity affect time velocity affect space geometry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000042bf85aa498cd78e</td>\n",
              "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
              "      <td>0</td>\n",
              "      <td>otto von guericke used magdeburg hemispheres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000455dfa3e01eae3af</td>\n",
              "      <td>Can I convert montra helicon D to a mountain bike by just changing the tyres?</td>\n",
              "      <td>0</td>\n",
              "      <td>convert montra helicon mountain bike changing tyres</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    qid  ...                             word_tokens_preprocessed\n",
              "0  00002165364db923c7e6  ...        quebec nationalists see province nation 1960s\n",
              "1  000032939017120e6e44  ...    adopted dog would encourage people adopt not shop\n",
              "2  0000412ca6e4628ce2cf  ...  velocity affect time velocity affect space geometry\n",
              "3  000042bf85aa498cd78e  ...         otto von guericke used magdeburg hemispheres\n",
              "4  0000455dfa3e01eae3af  ...  convert montra helicon mountain bike changing tyres\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "text": [
            "time: 6min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXJUsadqpFYi",
        "colab_type": "text"
      },
      "source": [
        "We will now use Gensim implementation of Word2Vec and train it on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53RTe1UCEUF",
        "colab_type": "code",
        "outputId": "d9094d25-d6ec-4a6d-e73b-b53e9ab90b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import gzip\n",
        "import gensim \n",
        "import logging\n",
        " \n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# build vocabulary and train model\n",
        "wv = gensim.models.Word2Vec(\n",
        "        df['word_tokens_preprocessed'],\n",
        "        size=50,\n",
        "        window=10,\n",
        "        min_count=2,\n",
        "        workers=10,\n",
        "        iter=10)\n",
        " \n",
        " \n",
        " "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-21 01:52:23,808 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
            "2019-11-21 01:52:23,811 : INFO : collecting all words and their counts\n",
            "2019-11-21 01:52:23,814 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "2019-11-21 01:52:23,817 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-11-21 01:52:23,872 : INFO : PROGRESS: at sentence #10000, processed 449146 words, keeping 37 word types\n",
            "2019-11-21 01:52:23,926 : INFO : PROGRESS: at sentence #20000, processed 898577 words, keeping 37 word types\n",
            "2019-11-21 01:52:23,979 : INFO : PROGRESS: at sentence #30000, processed 1347176 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,033 : INFO : PROGRESS: at sentence #40000, processed 1795341 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,086 : INFO : PROGRESS: at sentence #50000, processed 2243592 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,137 : INFO : PROGRESS: at sentence #60000, processed 2693035 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,189 : INFO : PROGRESS: at sentence #70000, processed 3145671 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,239 : INFO : PROGRESS: at sentence #80000, processed 3596631 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,289 : INFO : PROGRESS: at sentence #90000, processed 4049304 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,340 : INFO : PROGRESS: at sentence #100000, processed 4497023 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,391 : INFO : PROGRESS: at sentence #110000, processed 4945730 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,440 : INFO : PROGRESS: at sentence #120000, processed 5395242 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,491 : INFO : PROGRESS: at sentence #130000, processed 5843099 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,541 : INFO : PROGRESS: at sentence #140000, processed 6294378 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,589 : INFO : PROGRESS: at sentence #150000, processed 6744265 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,637 : INFO : PROGRESS: at sentence #160000, processed 7193175 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,688 : INFO : PROGRESS: at sentence #170000, processed 7643998 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,747 : INFO : PROGRESS: at sentence #180000, processed 8095180 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,800 : INFO : PROGRESS: at sentence #190000, processed 8545433 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,854 : INFO : PROGRESS: at sentence #200000, processed 8997228 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,904 : INFO : PROGRESS: at sentence #210000, processed 9447686 words, keeping 37 word types\n",
            "2019-11-21 01:52:24,952 : INFO : PROGRESS: at sentence #220000, processed 9897458 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,001 : INFO : PROGRESS: at sentence #230000, processed 10346982 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,051 : INFO : PROGRESS: at sentence #240000, processed 10797775 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,099 : INFO : PROGRESS: at sentence #250000, processed 11246136 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,147 : INFO : PROGRESS: at sentence #260000, processed 11693455 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,195 : INFO : PROGRESS: at sentence #270000, processed 12145644 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,244 : INFO : PROGRESS: at sentence #280000, processed 12598698 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,292 : INFO : PROGRESS: at sentence #290000, processed 13047798 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,342 : INFO : PROGRESS: at sentence #300000, processed 13496635 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,390 : INFO : PROGRESS: at sentence #310000, processed 13949341 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,438 : INFO : PROGRESS: at sentence #320000, processed 14402079 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,486 : INFO : PROGRESS: at sentence #330000, processed 14852316 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,535 : INFO : PROGRESS: at sentence #340000, processed 15302065 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,583 : INFO : PROGRESS: at sentence #350000, processed 15755582 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,631 : INFO : PROGRESS: at sentence #360000, processed 16205870 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,679 : INFO : PROGRESS: at sentence #370000, processed 16656756 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,728 : INFO : PROGRESS: at sentence #380000, processed 17110065 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,775 : INFO : PROGRESS: at sentence #390000, processed 17560547 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,823 : INFO : PROGRESS: at sentence #400000, processed 18009214 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,870 : INFO : PROGRESS: at sentence #410000, processed 18454532 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,927 : INFO : PROGRESS: at sentence #420000, processed 18905418 words, keeping 37 word types\n",
            "2019-11-21 01:52:25,974 : INFO : PROGRESS: at sentence #430000, processed 19359532 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,022 : INFO : PROGRESS: at sentence #440000, processed 19810298 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,068 : INFO : PROGRESS: at sentence #450000, processed 20254989 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,116 : INFO : PROGRESS: at sentence #460000, processed 20703799 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,163 : INFO : PROGRESS: at sentence #470000, processed 21152580 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,210 : INFO : PROGRESS: at sentence #480000, processed 21600995 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,258 : INFO : PROGRESS: at sentence #490000, processed 22049055 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,305 : INFO : PROGRESS: at sentence #500000, processed 22496530 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,353 : INFO : PROGRESS: at sentence #510000, processed 22945866 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,401 : INFO : PROGRESS: at sentence #520000, processed 23394658 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,449 : INFO : PROGRESS: at sentence #530000, processed 23847648 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,499 : INFO : PROGRESS: at sentence #540000, processed 24297125 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,549 : INFO : PROGRESS: at sentence #550000, processed 24750186 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,597 : INFO : PROGRESS: at sentence #560000, processed 25197457 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,645 : INFO : PROGRESS: at sentence #570000, processed 25651228 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,693 : INFO : PROGRESS: at sentence #580000, processed 26100778 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,741 : INFO : PROGRESS: at sentence #590000, processed 26551560 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,789 : INFO : PROGRESS: at sentence #600000, processed 27005897 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,838 : INFO : PROGRESS: at sentence #610000, processed 27458357 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,886 : INFO : PROGRESS: at sentence #620000, processed 27908929 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,933 : INFO : PROGRESS: at sentence #630000, processed 28359898 words, keeping 37 word types\n",
            "2019-11-21 01:52:26,980 : INFO : PROGRESS: at sentence #640000, processed 28808410 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,030 : INFO : PROGRESS: at sentence #650000, processed 29262984 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,079 : INFO : PROGRESS: at sentence #660000, processed 29712329 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,129 : INFO : PROGRESS: at sentence #670000, processed 30164395 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,176 : INFO : PROGRESS: at sentence #680000, processed 30612055 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,223 : INFO : PROGRESS: at sentence #690000, processed 31061139 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,271 : INFO : PROGRESS: at sentence #700000, processed 31514646 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,320 : INFO : PROGRESS: at sentence #710000, processed 31962823 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,369 : INFO : PROGRESS: at sentence #720000, processed 32413386 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,416 : INFO : PROGRESS: at sentence #730000, processed 32860175 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,464 : INFO : PROGRESS: at sentence #740000, processed 33313493 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,511 : INFO : PROGRESS: at sentence #750000, processed 33758576 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,558 : INFO : PROGRESS: at sentence #760000, processed 34205730 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,606 : INFO : PROGRESS: at sentence #770000, processed 34658241 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,654 : INFO : PROGRESS: at sentence #780000, processed 35106958 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,702 : INFO : PROGRESS: at sentence #790000, processed 35552440 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,753 : INFO : PROGRESS: at sentence #800000, processed 35995049 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,803 : INFO : PROGRESS: at sentence #810000, processed 36444627 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,853 : INFO : PROGRESS: at sentence #820000, processed 36897818 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,903 : INFO : PROGRESS: at sentence #830000, processed 37351236 words, keeping 37 word types\n",
            "2019-11-21 01:52:27,966 : INFO : PROGRESS: at sentence #840000, processed 37803517 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,022 : INFO : PROGRESS: at sentence #850000, processed 38257690 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,072 : INFO : PROGRESS: at sentence #860000, processed 38706355 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,121 : INFO : PROGRESS: at sentence #870000, processed 39158551 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,169 : INFO : PROGRESS: at sentence #880000, processed 39608954 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,216 : INFO : PROGRESS: at sentence #890000, processed 40059742 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,264 : INFO : PROGRESS: at sentence #900000, processed 40506463 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,311 : INFO : PROGRESS: at sentence #910000, processed 40956862 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,361 : INFO : PROGRESS: at sentence #920000, processed 41405059 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,409 : INFO : PROGRESS: at sentence #930000, processed 41851473 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,456 : INFO : PROGRESS: at sentence #940000, processed 42302841 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,504 : INFO : PROGRESS: at sentence #950000, processed 42755987 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,552 : INFO : PROGRESS: at sentence #960000, processed 43205457 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,600 : INFO : PROGRESS: at sentence #970000, processed 43654607 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,648 : INFO : PROGRESS: at sentence #980000, processed 44105980 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,695 : INFO : PROGRESS: at sentence #990000, processed 44556694 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,745 : INFO : PROGRESS: at sentence #1000000, processed 45010136 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,794 : INFO : PROGRESS: at sentence #1010000, processed 45458991 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,844 : INFO : PROGRESS: at sentence #1020000, processed 45908334 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,893 : INFO : PROGRESS: at sentence #1030000, processed 46358937 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,942 : INFO : PROGRESS: at sentence #1040000, processed 46811592 words, keeping 37 word types\n",
            "2019-11-21 01:52:28,990 : INFO : PROGRESS: at sentence #1050000, processed 47261992 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,038 : INFO : PROGRESS: at sentence #1060000, processed 47711005 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,088 : INFO : PROGRESS: at sentence #1070000, processed 48156958 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,140 : INFO : PROGRESS: at sentence #1080000, processed 48608938 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,189 : INFO : PROGRESS: at sentence #1090000, processed 49059470 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,238 : INFO : PROGRESS: at sentence #1100000, processed 49509740 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,286 : INFO : PROGRESS: at sentence #1110000, processed 49964058 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,335 : INFO : PROGRESS: at sentence #1120000, processed 50413811 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,383 : INFO : PROGRESS: at sentence #1130000, processed 50863153 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,437 : INFO : PROGRESS: at sentence #1140000, processed 51312295 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,485 : INFO : PROGRESS: at sentence #1150000, processed 51762914 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,534 : INFO : PROGRESS: at sentence #1160000, processed 52213337 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,582 : INFO : PROGRESS: at sentence #1170000, processed 52662666 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,630 : INFO : PROGRESS: at sentence #1180000, processed 53111423 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,679 : INFO : PROGRESS: at sentence #1190000, processed 53562235 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,729 : INFO : PROGRESS: at sentence #1200000, processed 54014183 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,778 : INFO : PROGRESS: at sentence #1210000, processed 54467072 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,827 : INFO : PROGRESS: at sentence #1220000, processed 54921598 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,876 : INFO : PROGRESS: at sentence #1230000, processed 55371293 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,924 : INFO : PROGRESS: at sentence #1240000, processed 55820238 words, keeping 37 word types\n",
            "2019-11-21 01:52:29,973 : INFO : PROGRESS: at sentence #1250000, processed 56268500 words, keeping 37 word types\n",
            "2019-11-21 01:52:30,022 : INFO : PROGRESS: at sentence #1260000, processed 56716219 words, keeping 37 word types\n",
            "2019-11-21 01:52:30,074 : INFO : PROGRESS: at sentence #1270000, processed 57168075 words, keeping 37 word types\n",
            "2019-11-21 01:52:30,123 : INFO : PROGRESS: at sentence #1280000, processed 57622200 words, keeping 37 word types\n",
            "2019-11-21 01:52:30,171 : INFO : PROGRESS: at sentence #1290000, processed 58067097 words, keeping 37 word types\n",
            "2019-11-21 01:52:30,220 : INFO : PROGRESS: at sentence #1300000, processed 58516141 words, keeping 37 word types\n",
            "2019-11-21 01:52:30,254 : INFO : collected 37 word types from a corpus of 58793789 raw words and 1306122 sentences\n",
            "2019-11-21 01:52:30,255 : INFO : Loading a fresh vocabulary\n",
            "2019-11-21 01:52:30,257 : INFO : effective_min_count=2 retains 37 unique words (100% of original 37, drops 0)\n",
            "2019-11-21 01:52:30,258 : INFO : effective_min_count=2 leaves 58793789 word corpus (100% of original 58793789, drops 0)\n",
            "2019-11-21 01:52:30,260 : INFO : deleting the raw counts dictionary of 37 items\n",
            "2019-11-21 01:52:30,261 : INFO : sample=0.001 downsamples 24 most-common words\n",
            "2019-11-21 01:52:30,262 : INFO : downsampling leaves estimated 10582612 word corpus (18.0% of prior 58793789)\n",
            "2019-11-21 01:52:30,265 : INFO : estimated required memory for 37 words and 50 dimensions: 33300 bytes\n",
            "2019-11-21 01:52:30,268 : INFO : resetting layer weights\n",
            "2019-11-21 01:52:30,288 : INFO : training model with 10 workers on 37 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2019-11-21 01:52:31,307 : INFO : EPOCH 1 - PROGRESS: at 4.44% examples, 465644 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:32,308 : INFO : EPOCH 1 - PROGRESS: at 8.98% examples, 472829 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:33,308 : INFO : EPOCH 1 - PROGRESS: at 13.46% examples, 473464 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:34,315 : INFO : EPOCH 1 - PROGRESS: at 17.75% examples, 467904 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:35,318 : INFO : EPOCH 1 - PROGRESS: at 22.11% examples, 466255 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:36,321 : INFO : EPOCH 1 - PROGRESS: at 26.54% examples, 466568 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:37,331 : INFO : EPOCH 1 - PROGRESS: at 31.02% examples, 466949 words/s, in_qsize 16, out_qsize 2\n",
            "2019-11-21 01:52:38,335 : INFO : EPOCH 1 - PROGRESS: at 35.57% examples, 468424 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:52:39,339 : INFO : EPOCH 1 - PROGRESS: at 39.91% examples, 467041 words/s, in_qsize 19, out_qsize 2\n",
            "2019-11-21 01:52:40,345 : INFO : EPOCH 1 - PROGRESS: at 44.41% examples, 467880 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:52:41,345 : INFO : EPOCH 1 - PROGRESS: at 48.89% examples, 468603 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:52:42,349 : INFO : EPOCH 1 - PROGRESS: at 53.41% examples, 469290 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:43,353 : INFO : EPOCH 1 - PROGRESS: at 57.92% examples, 469605 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:52:44,357 : INFO : EPOCH 1 - PROGRESS: at 62.28% examples, 468820 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:45,357 : INFO : EPOCH 1 - PROGRESS: at 66.72% examples, 468981 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:52:46,359 : INFO : EPOCH 1 - PROGRESS: at 71.26% examples, 469488 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:47,366 : INFO : EPOCH 1 - PROGRESS: at 75.84% examples, 470191 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:48,377 : INFO : EPOCH 1 - PROGRESS: at 80.34% examples, 470223 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:49,381 : INFO : EPOCH 1 - PROGRESS: at 84.95% examples, 471103 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:50,387 : INFO : EPOCH 1 - PROGRESS: at 89.33% examples, 470539 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:51,388 : INFO : EPOCH 1 - PROGRESS: at 93.81% examples, 470734 words/s, in_qsize 18, out_qsize 2\n",
            "2019-11-21 01:52:52,390 : INFO : EPOCH 1 - PROGRESS: at 98.32% examples, 470960 words/s, in_qsize 19, out_qsize 2\n",
            "2019-11-21 01:52:52,733 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:52:52,735 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:52:52,741 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:52:52,743 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:52:52,750 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:52:52,754 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:52:52,766 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:52:52,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:52:52,776 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:52:52,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:52:52,779 : INFO : EPOCH - 1 : training on 58793789 raw words (10578524 effective words) took 22.5s, 470692 effective words/s\n",
            "2019-11-21 01:52:53,797 : INFO : EPOCH 2 - PROGRESS: at 4.49% examples, 472287 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:54,797 : INFO : EPOCH 2 - PROGRESS: at 8.84% examples, 466526 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:55,799 : INFO : EPOCH 2 - PROGRESS: at 13.38% examples, 470743 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:56,804 : INFO : EPOCH 2 - PROGRESS: at 17.92% examples, 472611 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:52:57,804 : INFO : EPOCH 2 - PROGRESS: at 22.43% examples, 473589 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:58,807 : INFO : EPOCH 2 - PROGRESS: at 26.88% examples, 473092 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:52:59,810 : INFO : EPOCH 2 - PROGRESS: at 31.39% examples, 473535 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:00,814 : INFO : EPOCH 2 - PROGRESS: at 35.89% examples, 473672 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:01,815 : INFO : EPOCH 2 - PROGRESS: at 40.38% examples, 473576 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:02,816 : INFO : EPOCH 2 - PROGRESS: at 44.92% examples, 474201 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:03,821 : INFO : EPOCH 2 - PROGRESS: at 49.43% examples, 474424 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:04,825 : INFO : EPOCH 2 - PROGRESS: at 53.84% examples, 473547 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:05,830 : INFO : EPOCH 2 - PROGRESS: at 58.42% examples, 474047 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:06,836 : INFO : EPOCH 2 - PROGRESS: at 62.92% examples, 474024 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:07,838 : INFO : EPOCH 2 - PROGRESS: at 67.45% examples, 474430 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:08,838 : INFO : EPOCH 2 - PROGRESS: at 71.99% examples, 474669 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:09,843 : INFO : EPOCH 2 - PROGRESS: at 76.56% examples, 475114 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:10,852 : INFO : EPOCH 2 - PROGRESS: at 81.02% examples, 474676 words/s, in_qsize 17, out_qsize 1\n",
            "2019-11-21 01:53:11,855 : INFO : EPOCH 2 - PROGRESS: at 85.59% examples, 475112 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:12,859 : INFO : EPOCH 2 - PROGRESS: at 90.14% examples, 475330 words/s, in_qsize 20, out_qsize 3\n",
            "2019-11-21 01:53:13,873 : INFO : EPOCH 2 - PROGRESS: at 94.72% examples, 475542 words/s, in_qsize 19, out_qsize 3\n",
            "2019-11-21 01:53:14,876 : INFO : EPOCH 2 - PROGRESS: at 99.24% examples, 475613 words/s, in_qsize 17, out_qsize 2\n",
            "2019-11-21 01:53:15,012 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:53:15,016 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:53:15,018 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:53:15,019 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:53:15,025 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:53:15,035 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:53:15,037 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:53:15,041 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:53:15,043 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:53:15,051 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:53:15,053 : INFO : EPOCH - 2 : training on 58793789 raw words (10582031 effective words) took 22.3s, 475440 effective words/s\n",
            "2019-11-21 01:53:16,076 : INFO : EPOCH 3 - PROGRESS: at 4.37% examples, 459677 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:17,077 : INFO : EPOCH 3 - PROGRESS: at 8.83% examples, 465634 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:18,085 : INFO : EPOCH 3 - PROGRESS: at 13.36% examples, 469225 words/s, in_qsize 16, out_qsize 3\n",
            "2019-11-21 01:53:19,089 : INFO : EPOCH 3 - PROGRESS: at 17.88% examples, 471116 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:20,092 : INFO : EPOCH 3 - PROGRESS: at 22.43% examples, 472803 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:21,094 : INFO : EPOCH 3 - PROGRESS: at 26.81% examples, 471386 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:53:22,097 : INFO : EPOCH 3 - PROGRESS: at 31.25% examples, 470859 words/s, in_qsize 18, out_qsize 3\n",
            "2019-11-21 01:53:23,112 : INFO : EPOCH 3 - PROGRESS: at 35.82% examples, 471478 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:24,113 : INFO : EPOCH 3 - PROGRESS: at 40.30% examples, 471486 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:25,120 : INFO : EPOCH 3 - PROGRESS: at 44.75% examples, 471286 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:26,125 : INFO : EPOCH 3 - PROGRESS: at 49.16% examples, 470665 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:27,126 : INFO : EPOCH 3 - PROGRESS: at 53.62% examples, 470715 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:53:28,129 : INFO : EPOCH 3 - PROGRESS: at 58.09% examples, 470747 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:29,137 : INFO : EPOCH 3 - PROGRESS: at 62.69% examples, 471411 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:30,140 : INFO : EPOCH 3 - PROGRESS: at 67.23% examples, 472066 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:31,140 : INFO : EPOCH 3 - PROGRESS: at 71.76% examples, 472366 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:32,143 : INFO : EPOCH 3 - PROGRESS: at 76.26% examples, 472557 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:33,146 : INFO : EPOCH 3 - PROGRESS: at 80.76% examples, 472721 words/s, in_qsize 17, out_qsize 2\n",
            "2019-11-21 01:53:34,152 : INFO : EPOCH 3 - PROGRESS: at 85.29% examples, 472882 words/s, in_qsize 19, out_qsize 4\n",
            "2019-11-21 01:53:35,155 : INFO : EPOCH 3 - PROGRESS: at 89.85% examples, 473300 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:53:36,158 : INFO : EPOCH 3 - PROGRESS: at 94.23% examples, 472851 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:37,161 : INFO : EPOCH 3 - PROGRESS: at 98.72% examples, 472797 words/s, in_qsize 17, out_qsize 2\n",
            "2019-11-21 01:53:37,403 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:53:37,408 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:53:37,415 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:53:37,419 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:53:37,431 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:53:37,437 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:53:37,440 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:53:37,441 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:53:37,443 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:53:37,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:53:37,448 : INFO : EPOCH - 3 : training on 58793789 raw words (10578864 effective words) took 22.4s, 472811 effective words/s\n",
            "2019-11-21 01:53:38,473 : INFO : EPOCH 4 - PROGRESS: at 4.47% examples, 469926 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:39,477 : INFO : EPOCH 4 - PROGRESS: at 8.93% examples, 469823 words/s, in_qsize 17, out_qsize 2\n",
            "2019-11-21 01:53:40,479 : INFO : EPOCH 4 - PROGRESS: at 13.40% examples, 470758 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:41,488 : INFO : EPOCH 4 - PROGRESS: at 17.90% examples, 471312 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:53:42,492 : INFO : EPOCH 4 - PROGRESS: at 22.38% examples, 471456 words/s, in_qsize 17, out_qsize 3\n",
            "2019-11-21 01:53:43,496 : INFO : EPOCH 4 - PROGRESS: at 26.94% examples, 473263 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:44,497 : INFO : EPOCH 4 - PROGRESS: at 31.41% examples, 473056 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:45,502 : INFO : EPOCH 4 - PROGRESS: at 35.89% examples, 472959 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:46,503 : INFO : EPOCH 4 - PROGRESS: at 40.44% examples, 473821 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:53:47,523 : INFO : EPOCH 4 - PROGRESS: at 44.92% examples, 472934 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:53:48,525 : INFO : EPOCH 4 - PROGRESS: at 49.57% examples, 474730 words/s, in_qsize 17, out_qsize 0\n",
            "2019-11-21 01:53:49,527 : INFO : EPOCH 4 - PROGRESS: at 54.02% examples, 474442 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:53:50,535 : INFO : EPOCH 4 - PROGRESS: at 58.55% examples, 474501 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:51,537 : INFO : EPOCH 4 - PROGRESS: at 63.00% examples, 474156 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:52,539 : INFO : EPOCH 4 - PROGRESS: at 67.48% examples, 474167 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:53,542 : INFO : EPOCH 4 - PROGRESS: at 72.06% examples, 474611 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:54,548 : INFO : EPOCH 4 - PROGRESS: at 76.56% examples, 474578 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:55,549 : INFO : EPOCH 4 - PROGRESS: at 81.10% examples, 474805 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:56,550 : INFO : EPOCH 4 - PROGRESS: at 85.62% examples, 475028 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:53:57,552 : INFO : EPOCH 4 - PROGRESS: at 90.06% examples, 474614 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:53:58,555 : INFO : EPOCH 4 - PROGRESS: at 94.54% examples, 474587 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:53:59,557 : INFO : EPOCH 4 - PROGRESS: at 99.05% examples, 474682 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:53:59,729 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:53:59,741 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:53:59,744 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:53:59,754 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:53:59,757 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:53:59,758 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:53:59,759 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:53:59,764 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:53:59,765 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:53:59,767 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:53:59,768 : INFO : EPOCH - 4 : training on 58793789 raw words (10584382 effective words) took 22.3s, 474677 effective words/s\n",
            "2019-11-21 01:54:00,798 : INFO : EPOCH 5 - PROGRESS: at 4.49% examples, 471771 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:01,799 : INFO : EPOCH 5 - PROGRESS: at 9.00% examples, 473798 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:02,807 : INFO : EPOCH 5 - PROGRESS: at 13.51% examples, 473699 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:03,808 : INFO : EPOCH 5 - PROGRESS: at 18.03% examples, 475184 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:04,808 : INFO : EPOCH 5 - PROGRESS: at 22.55% examples, 475390 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:05,812 : INFO : EPOCH 5 - PROGRESS: at 27.01% examples, 474841 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:06,816 : INFO : EPOCH 5 - PROGRESS: at 31.44% examples, 473765 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:07,820 : INFO : EPOCH 5 - PROGRESS: at 35.84% examples, 472620 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:08,821 : INFO : EPOCH 5 - PROGRESS: at 40.40% examples, 473457 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:09,825 : INFO : EPOCH 5 - PROGRESS: at 44.93% examples, 473997 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:10,825 : INFO : EPOCH 5 - PROGRESS: at 49.37% examples, 473581 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:11,830 : INFO : EPOCH 5 - PROGRESS: at 53.80% examples, 473133 words/s, in_qsize 20, out_qsize 2\n",
            "2019-11-21 01:54:12,831 : INFO : EPOCH 5 - PROGRESS: at 58.35% examples, 473560 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:13,834 : INFO : EPOCH 5 - PROGRESS: at 62.92% examples, 474132 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:14,834 : INFO : EPOCH 5 - PROGRESS: at 67.39% examples, 474147 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:15,837 : INFO : EPOCH 5 - PROGRESS: at 71.84% examples, 473818 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:54:16,839 : INFO : EPOCH 5 - PROGRESS: at 76.34% examples, 473951 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:54:17,842 : INFO : EPOCH 5 - PROGRESS: at 80.70% examples, 473213 words/s, in_qsize 17, out_qsize 2\n",
            "2019-11-21 01:54:18,849 : INFO : EPOCH 5 - PROGRESS: at 85.22% examples, 473336 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:54:19,870 : INFO : EPOCH 5 - PROGRESS: at 89.70% examples, 472918 words/s, in_qsize 19, out_qsize 3\n",
            "2019-11-21 01:54:20,874 : INFO : EPOCH 5 - PROGRESS: at 94.20% examples, 472962 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:21,878 : INFO : EPOCH 5 - PROGRESS: at 98.66% examples, 472824 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:22,130 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:54:22,137 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:54:22,146 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:54:22,152 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:54:22,158 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:54:22,173 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:54:22,175 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:54:22,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:54:22,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:54:22,179 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:54:22,181 : INFO : EPOCH - 5 : training on 58793789 raw words (10582735 effective words) took 22.4s, 472744 effective words/s\n",
            "2019-11-21 01:54:23,200 : INFO : EPOCH 6 - PROGRESS: at 4.47% examples, 471426 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:24,204 : INFO : EPOCH 6 - PROGRESS: at 8.95% examples, 471522 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:25,207 : INFO : EPOCH 6 - PROGRESS: at 13.35% examples, 469226 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:26,208 : INFO : EPOCH 6 - PROGRESS: at 17.87% examples, 471668 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:54:27,211 : INFO : EPOCH 6 - PROGRESS: at 22.38% examples, 472367 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:54:28,219 : INFO : EPOCH 6 - PROGRESS: at 26.89% examples, 472662 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:29,222 : INFO : EPOCH 6 - PROGRESS: at 31.31% examples, 471647 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:54:30,227 : INFO : EPOCH 6 - PROGRESS: at 35.81% examples, 472022 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:31,230 : INFO : EPOCH 6 - PROGRESS: at 40.31% examples, 472240 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:54:32,234 : INFO : EPOCH 6 - PROGRESS: at 44.78% examples, 472306 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:33,234 : INFO : EPOCH 6 - PROGRESS: at 49.25% examples, 472400 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:34,242 : INFO : EPOCH 6 - PROGRESS: at 53.77% examples, 472622 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:35,244 : INFO : EPOCH 6 - PROGRESS: at 58.30% examples, 472997 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:36,247 : INFO : EPOCH 6 - PROGRESS: at 62.82% examples, 473222 words/s, in_qsize 18, out_qsize 0\n",
            "2019-11-21 01:54:37,250 : INFO : EPOCH 6 - PROGRESS: at 67.32% examples, 473292 words/s, in_qsize 18, out_qsize 0\n",
            "2019-11-21 01:54:38,253 : INFO : EPOCH 6 - PROGRESS: at 71.83% examples, 473387 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:39,254 : INFO : EPOCH 6 - PROGRESS: at 76.14% examples, 472409 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:40,257 : INFO : EPOCH 6 - PROGRESS: at 80.46% examples, 471454 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:54:41,259 : INFO : EPOCH 6 - PROGRESS: at 84.89% examples, 471266 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:42,261 : INFO : EPOCH 6 - PROGRESS: at 89.40% examples, 471562 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:43,261 : INFO : EPOCH 6 - PROGRESS: at 93.81% examples, 471350 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:44,262 : INFO : EPOCH 6 - PROGRESS: at 98.28% examples, 471404 words/s, in_qsize 20, out_qsize 3\n",
            "2019-11-21 01:54:44,601 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:54:44,607 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:54:44,615 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:54:44,627 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:54:44,628 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:54:44,629 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:54:44,635 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:54:44,644 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:54:44,646 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:54:44,648 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:54:44,649 : INFO : EPOCH - 6 : training on 58793789 raw words (10581227 effective words) took 22.5s, 471311 effective words/s\n",
            "2019-11-21 01:54:45,667 : INFO : EPOCH 7 - PROGRESS: at 4.40% examples, 465082 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:46,670 : INFO : EPOCH 7 - PROGRESS: at 8.91% examples, 469754 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:47,671 : INFO : EPOCH 7 - PROGRESS: at 13.42% examples, 471673 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:48,677 : INFO : EPOCH 7 - PROGRESS: at 17.93% examples, 472955 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:54:49,678 : INFO : EPOCH 7 - PROGRESS: at 22.31% examples, 470790 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:50,679 : INFO : EPOCH 7 - PROGRESS: at 26.78% examples, 471172 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:51,687 : INFO : EPOCH 7 - PROGRESS: at 31.27% examples, 471194 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:52,692 : INFO : EPOCH 7 - PROGRESS: at 35.76% examples, 471300 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:53,695 : INFO : EPOCH 7 - PROGRESS: at 40.30% examples, 471967 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:54,697 : INFO : EPOCH 7 - PROGRESS: at 44.75% examples, 471939 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:55,697 : INFO : EPOCH 7 - PROGRESS: at 49.16% examples, 471611 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:56,700 : INFO : EPOCH 7 - PROGRESS: at 53.62% examples, 471554 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:54:57,704 : INFO : EPOCH 7 - PROGRESS: at 58.09% examples, 471444 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:54:58,704 : INFO : EPOCH 7 - PROGRESS: at 62.62% examples, 471909 words/s, in_qsize 16, out_qsize 3\n",
            "2019-11-21 01:54:59,707 : INFO : EPOCH 7 - PROGRESS: at 67.06% examples, 471857 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:00,709 : INFO : EPOCH 7 - PROGRESS: at 71.61% examples, 472169 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:01,710 : INFO : EPOCH 7 - PROGRESS: at 76.09% examples, 472283 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:02,713 : INFO : EPOCH 7 - PROGRESS: at 80.51% examples, 472000 words/s, in_qsize 17, out_qsize 2\n",
            "2019-11-21 01:55:03,713 : INFO : EPOCH 7 - PROGRESS: at 84.98% examples, 472071 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:04,718 : INFO : EPOCH 7 - PROGRESS: at 89.41% examples, 471806 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:05,721 : INFO : EPOCH 7 - PROGRESS: at 93.93% examples, 472035 words/s, in_qsize 17, out_qsize 2\n",
            "2019-11-21 01:55:06,721 : INFO : EPOCH 7 - PROGRESS: at 98.35% examples, 471874 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:07,045 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:55:07,048 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:55:07,050 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:55:07,058 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:55:07,070 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:55:07,071 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:55:07,077 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:55:07,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:55:07,085 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:55:07,087 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:55:07,089 : INFO : EPOCH - 7 : training on 58793789 raw words (10581206 effective words) took 22.4s, 471894 effective words/s\n",
            "2019-11-21 01:55:08,112 : INFO : EPOCH 8 - PROGRESS: at 4.42% examples, 466009 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:09,112 : INFO : EPOCH 8 - PROGRESS: at 8.89% examples, 469731 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:10,117 : INFO : EPOCH 8 - PROGRESS: at 13.26% examples, 466407 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:11,119 : INFO : EPOCH 8 - PROGRESS: at 17.59% examples, 464232 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:55:12,119 : INFO : EPOCH 8 - PROGRESS: at 22.09% examples, 466375 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:55:13,120 : INFO : EPOCH 8 - PROGRESS: at 26.61% examples, 468483 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:14,124 : INFO : EPOCH 8 - PROGRESS: at 31.12% examples, 469431 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:15,134 : INFO : EPOCH 8 - PROGRESS: at 35.60% examples, 469511 words/s, in_qsize 16, out_qsize 3\n",
            "2019-11-21 01:55:16,137 : INFO : EPOCH 8 - PROGRESS: at 40.21% examples, 471135 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:17,138 : INFO : EPOCH 8 - PROGRESS: at 44.71% examples, 471819 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:18,141 : INFO : EPOCH 8 - PROGRESS: at 49.18% examples, 471729 words/s, in_qsize 18, out_qsize 0\n",
            "2019-11-21 01:55:19,143 : INFO : EPOCH 8 - PROGRESS: at 53.69% examples, 472201 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:20,156 : INFO : EPOCH 8 - PROGRESS: at 58.13% examples, 471483 words/s, in_qsize 14, out_qsize 5\n",
            "2019-11-21 01:55:21,156 : INFO : EPOCH 8 - PROGRESS: at 62.67% examples, 472019 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:22,158 : INFO : EPOCH 8 - PROGRESS: at 67.10% examples, 471790 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:23,159 : INFO : EPOCH 8 - PROGRESS: at 71.62% examples, 472062 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:24,161 : INFO : EPOCH 8 - PROGRESS: at 76.09% examples, 472097 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:55:25,163 : INFO : EPOCH 8 - PROGRESS: at 80.37% examples, 471058 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:26,166 : INFO : EPOCH 8 - PROGRESS: at 84.57% examples, 469539 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:27,168 : INFO : EPOCH 8 - PROGRESS: at 89.01% examples, 469491 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:28,170 : INFO : EPOCH 8 - PROGRESS: at 93.33% examples, 468956 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:55:29,178 : INFO : EPOCH 8 - PROGRESS: at 97.69% examples, 468471 words/s, in_qsize 20, out_qsize 2\n",
            "2019-11-21 01:55:29,657 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:55:29,661 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:55:29,679 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:55:29,681 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:55:29,687 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:55:29,697 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:55:29,701 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:55:29,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:55:29,704 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:55:29,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:55:29,707 : INFO : EPOCH - 8 : training on 58793789 raw words (10580820 effective words) took 22.6s, 468264 effective words/s\n",
            "2019-11-21 01:55:30,723 : INFO : EPOCH 9 - PROGRESS: at 4.22% examples, 443234 words/s, in_qsize 17, out_qsize 2\n",
            "2019-11-21 01:55:31,727 : INFO : EPOCH 9 - PROGRESS: at 8.76% examples, 460965 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:55:32,730 : INFO : EPOCH 9 - PROGRESS: at 13.23% examples, 464591 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:33,730 : INFO : EPOCH 9 - PROGRESS: at 17.76% examples, 468436 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:34,731 : INFO : EPOCH 9 - PROGRESS: at 22.16% examples, 467649 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:35,732 : INFO : EPOCH 9 - PROGRESS: at 26.73% examples, 470227 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:36,738 : INFO : EPOCH 9 - PROGRESS: at 31.17% examples, 469757 words/s, in_qsize 18, out_qsize 3\n",
            "2019-11-21 01:55:37,738 : INFO : EPOCH 9 - PROGRESS: at 35.63% examples, 470122 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:38,749 : INFO : EPOCH 9 - PROGRESS: at 40.18% examples, 470479 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:39,752 : INFO : EPOCH 9 - PROGRESS: at 44.75% examples, 471726 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:55:40,755 : INFO : EPOCH 9 - PROGRESS: at 49.01% examples, 469687 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:41,768 : INFO : EPOCH 9 - PROGRESS: at 53.31% examples, 467895 words/s, in_qsize 20, out_qsize 4\n",
            "2019-11-21 01:55:42,768 : INFO : EPOCH 9 - PROGRESS: at 57.92% examples, 469310 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:43,770 : INFO : EPOCH 9 - PROGRESS: at 62.45% examples, 469936 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:44,772 : INFO : EPOCH 9 - PROGRESS: at 67.00% examples, 470728 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:45,772 : INFO : EPOCH 9 - PROGRESS: at 71.50% examples, 470953 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:46,781 : INFO : EPOCH 9 - PROGRESS: at 76.04% examples, 471329 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:47,783 : INFO : EPOCH 9 - PROGRESS: at 80.58% examples, 471792 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:55:48,785 : INFO : EPOCH 9 - PROGRESS: at 85.10% examples, 472160 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:49,791 : INFO : EPOCH 9 - PROGRESS: at 89.67% examples, 472536 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:55:50,792 : INFO : EPOCH 9 - PROGRESS: at 94.10% examples, 472425 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:51,798 : INFO : EPOCH 9 - PROGRESS: at 98.54% examples, 472235 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:52,098 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:55:52,100 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:55:52,104 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:55:52,112 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:55:52,116 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:55:52,119 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:55:52,129 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:55:52,135 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:55:52,140 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:55:52,142 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:55:52,143 : INFO : EPOCH - 9 : training on 58793789 raw words (10580249 effective words) took 22.4s, 471814 effective words/s\n",
            "2019-11-21 01:55:53,163 : INFO : EPOCH 10 - PROGRESS: at 4.49% examples, 473432 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:54,167 : INFO : EPOCH 10 - PROGRESS: at 8.93% examples, 470330 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:55,170 : INFO : EPOCH 10 - PROGRESS: at 13.43% examples, 472124 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:56,171 : INFO : EPOCH 10 - PROGRESS: at 17.82% examples, 470427 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:57,173 : INFO : EPOCH 10 - PROGRESS: at 22.31% examples, 471032 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:58,195 : INFO : EPOCH 10 - PROGRESS: at 26.79% examples, 470004 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:55:59,200 : INFO : EPOCH 10 - PROGRESS: at 31.32% examples, 471030 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:56:00,216 : INFO : EPOCH 10 - PROGRESS: at 35.89% examples, 471554 words/s, in_qsize 19, out_qsize 3\n",
            "2019-11-21 01:56:01,219 : INFO : EPOCH 10 - PROGRESS: at 40.46% examples, 472721 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:56:02,221 : INFO : EPOCH 10 - PROGRESS: at 44.85% examples, 471839 words/s, in_qsize 20, out_qsize 1\n",
            "2019-11-21 01:56:03,224 : INFO : EPOCH 10 - PROGRESS: at 49.33% examples, 472087 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:56:04,230 : INFO : EPOCH 10 - PROGRESS: at 53.87% examples, 472543 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:56:05,230 : INFO : EPOCH 10 - PROGRESS: at 58.36% examples, 472702 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:56:06,236 : INFO : EPOCH 10 - PROGRESS: at 62.82% examples, 472407 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:56:07,243 : INFO : EPOCH 10 - PROGRESS: at 67.34% examples, 472591 words/s, in_qsize 18, out_qsize 1\n",
            "2019-11-21 01:56:08,246 : INFO : EPOCH 10 - PROGRESS: at 71.89% examples, 473076 words/s, in_qsize 19, out_qsize 1\n",
            "2019-11-21 01:56:09,248 : INFO : EPOCH 10 - PROGRESS: at 76.43% examples, 473576 words/s, in_qsize 19, out_qsize 1\n",
            "2019-11-21 01:56:10,253 : INFO : EPOCH 10 - PROGRESS: at 80.95% examples, 473747 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:56:11,262 : INFO : EPOCH 10 - PROGRESS: at 85.40% examples, 473475 words/s, in_qsize 20, out_qsize 0\n",
            "2019-11-21 01:56:12,263 : INFO : EPOCH 10 - PROGRESS: at 89.85% examples, 473298 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:56:13,269 : INFO : EPOCH 10 - PROGRESS: at 94.37% examples, 473422 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:56:14,273 : INFO : EPOCH 10 - PROGRESS: at 98.97% examples, 473926 words/s, in_qsize 19, out_qsize 0\n",
            "2019-11-21 01:56:14,472 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2019-11-21 01:56:14,476 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2019-11-21 01:56:14,487 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2019-11-21 01:56:14,492 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2019-11-21 01:56:14,497 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2019-11-21 01:56:14,506 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2019-11-21 01:56:14,507 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-11-21 01:56:14,510 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-21 01:56:14,512 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-21 01:56:14,513 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-21 01:56:14,514 : INFO : EPOCH - 10 : training on 58793789 raw words (10588405 effective words) took 22.4s, 473696 effective words/s\n",
            "2019-11-21 01:56:14,515 : INFO : training on a 587937890 raw words (105818443 effective words) took 224.2s, 471928 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time: 3min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTpTa2Be3k9T",
        "colab_type": "text"
      },
      "source": [
        "The code  gets the average of 300 dimension word vectors for every word in the question. So, the averaged word vector is a word vector for the question as a whole. This code is highly efficient since it's all on numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MQWkXUNzfK1",
        "colab_type": "code",
        "outputId": "0f2646d1-da89-45df-d0cd-96f69fc7e6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#below code takes ~ 3 mins for the entire dataset.\n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    \n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "    \n",
        "    for word in words.split(' '):\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "    \n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "    \n",
        "   \n",
        "def averaged_word_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "# get document level embeddings\n",
        "w2v_feature_array = averaged_word_vectorizer(corpus=df['word_tokens_preprocessed'], model=wv,\n",
        "                                             num_features=50)\n",
        "question_vectors=pd.DataFrame(w2v_feature_array)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 8.81 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtw2Zs3J3w2u",
        "colab_type": "text"
      },
      "source": [
        "Verify the embeddings are generated for every sample in the dataset. Split up the dataset into training and testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJlDB8xizkM9",
        "colab_type": "code",
        "outputId": "0e5de8d0-4994-4065-e0d6-c10bad869696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "assert len(question_vectors)==len(df)\n",
        "\n",
        "question_vectors['target']=df['target']\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(question_vectors.drop('target',axis=1),question_vectors['target'], random_state=42,stratify=df['target'])\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((979591, 50), (979591,), (326531, 50), (326531,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "text": [
            "time: 1.14 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-CxLsI94BMr",
        "colab_type": "text"
      },
      "source": [
        "Train and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "455s3Gp7znGl",
        "colab_type": "code",
        "outputId": "ecc033cf-0fc2-4861-f5f2-baed059af95e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "# ~ 3.5 mins. Logistic Regression with class weight param\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc,precision_recall_curve,roc_curve,precision_recall_fscore_support\n",
        "\n",
        "#import as needed\n",
        "'''\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "'''\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "EST = timezone('US/Eastern')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "result_cols = [\"Classifier\", \"Accuracy\"]\n",
        "result_frame = pd.DataFrame(columns=result_cols)\n",
        "\n",
        "classifiers = [\n",
        "   # KNeighborsClassifier(2), # okay performance, takes 90 mins\n",
        "     \n",
        "   # RandomForestClassifier(), #performance not good.\n",
        "   # GradientBoostingClassifier(), #performance not good\n",
        "     #MultinomialNB() #performance not good.\n",
        "   # XGBClassifier(), #performance not good.\n",
        "    #LGBMClassifier(), #performance not good.\n",
        "    #DecisionTreeClassifier(),# best so far excluding logistic regression ( balanced weights) but .40 is the accuracy on the second label.\n",
        "    #LogisticRegression(),\n",
        "    LogisticRegression(class_weight='balanced')\n",
        "]\n",
        "  \n",
        "for clf in classifiers:\n",
        "     \n",
        "    name = clf.__class__.__name__\n",
        "    print(name)\n",
        "    text_clf = Pipeline([('clf', clf)])\n",
        "    text_clf.fit(x_train, y_train)\n",
        "    y_pred = text_clf.predict(x_test)\n",
        "    classifier_reports(y_test, y_pred)\n",
        "    \n",
        "    acc = metrics.accuracy_score(y_test,y_pred)\n",
        "    \n",
        "    print()\n",
        "    EST_Time = datetime.now(EST)\n",
        "    print(EST_Time.strftime(\"%a, %d %B %Y %H:%M:%S\"))\n",
        "    \n",
        "    acc_field = pd.DataFrame([[name, np.round(acc*100,2)]], columns=result_cols)\n",
        "    result_frame = result_frame.append(acc_field)  \n",
        "    \n",
        "    np.set_printoptions(precision=2)\n",
        "\n",
        "    \n",
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Accuracy', y='Classifier', data=result_frame, color=\"r\")\n",
        "\n",
        "plt.xlabel('Accuracy %')\n",
        "plt.title('Classifier Accuracy')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      class0       0.97      0.03      0.06    306328\n",
            "      class1       0.06      0.98      0.12     20203\n",
            "\n",
            "    accuracy                           0.09    326531\n",
            "   macro avg       0.52      0.51      0.09    326531\n",
            "weighted avg       0.91      0.09      0.07    326531\n",
            "\n",
            "\n",
            "Wed, 20 November 2019 21:47:47\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc8AAAEWCAYAAAAASRzMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neEqS1On/A86lXDsgkK74AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "time: 7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPSN6a__qS9w",
        "colab_type": "text"
      },
      "source": [
        "This is just an experiment done out of curiosity. In reality, for word2vec to perform well, it needs huge amount of corpus. Abysmal performance as seen above is due to the fact that the dataset isn't as huge as it needs to be for word2vec"
      ]
    }
  ]
}